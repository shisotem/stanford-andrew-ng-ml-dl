# Back Propagation (Optional)

## What is a derivative? (Optional)

> [!IMPORTANT]
>
> In TensorFlow, you specify a neural network architecture, f<sub>W,B</sub>(x), and a loss function, L(f<sub>W,B</sub>(x),y). Adam then trains the network parameters using the **derivatives computed by back propagation**.

- asdf
  ![alt text](resources/notes/01.png)
  ![alt text](resources/notes/02.png)
  ![alt text](resources/notes/03.png)
  ![alt text](resources/notes/04.png)
  ![alt text](resources/notes/05.png)
  ![alt text](resources/notes/06.png)
  ![alt text](resources/notes/07.png)

## Computation graph (Optional)

![alt text](resources/notes/08.png)
![alt text](resources/notes/09.png)
![alt text](resources/notes/10.png)
![alt text](resources/notes/11.png)

## Larger neural network example (Optional)

![alt text](resources/notes/12.png)
![alt text](resources/notes/13.png)
![alt text](resources/notes/14.png)

## Optional Lab: Derivatives

## Optional Lab: Back propagation
