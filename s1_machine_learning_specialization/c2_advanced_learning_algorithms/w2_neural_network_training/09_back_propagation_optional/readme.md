# Back Propagation (Optional)

## What is a derivative? (Optional)

> [!IMPORTANT]
>
> In TensorFlow, you specify a neural network architecture, f<sub>W,B</sub>(x), and a loss function, L(f<sub>W,B</sub>(x),y). Adam then trains the network parameters using the **derivatives computed by back propagation**.

- âˆ‚J(w)/âˆ‚w = 6: **"If w goes up by a tiny little amount Îµ, J(w) goes up 6 times as much."**

  ![alt text](resources/notes/01.png)

  ![alt text](resources/notes/02.png)

  ![alt text](resources/notes/03.png)

- Derivative examples:

  ![alt text](resources/notes/04.png)

  ![alt text](resources/notes/05.png)

  ![alt text](resources/notes/06.png)

- Notation:

  ![alt text](resources/notes/07.png)

## Computation graph (Optional)

- Forward prop:

  ![alt text](resources/notes/08.png)

- **Forward prop &rarr; (remember w, c, b, a, d, J) &rarr; Back prop**:

  - For example, if d increases from 2 to 2.001 (+0.001), then J increases from 2 to 2.0020005 (+0.002). Therefore, **âˆ‚J/âˆ‚d** = 2.

  - **Chain rule**: âˆ‚J/âˆ‚w = (((**âˆ‚J/âˆ‚d**) \* âˆ‚d/âˆ‚a) \* âˆ‚a/âˆ‚c) \* âˆ‚c/âˆ‚w

  ![alt text](resources/notes/09.png)

  - Manually verify that âˆ‚J/âˆ‚w equals -4:

  ![alt text](resources/notes/10.png)

- The efficiency of backprop:

  ![alt text](resources/notes/11.png)

> [!NOTE]
>
> - ðŸ¤¯ Numerical Differentiation: âˆ‚J(w)/âˆ‚w = (J(w+Îµ) - J(w-Îµ)) / 2Îµ. For example, as Îµ=1<sup>-4</sup>.
>
> - ðŸ¤¯ Symbolic Differentiation: both the input and output are **expressions**, not values. For instance, the derivative of x<sup>2</sup> with respect to x is 2x.
>
> - ðŸ˜Ž **Automatic Differentiation** (Back Propagation): use the **chain rule** to compute both derivatives **efficiently** and **accurately**.

## Larger neural network example (Optional)

- asdf

  ![alt text](resources/notes/12.png)

  ![alt text](resources/notes/13.png)

  ![alt text](resources/notes/14.png)

## Optional Lab: Derivatives

## Optional Lab: Back propagation
