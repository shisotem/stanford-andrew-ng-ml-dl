# Train the model with gradient descent

- Gradient descent is a **crucial** algorithm not only in linear regression, but also across **all areas of machine learning** (including deep learning).

- Gradient descent is an optimization algorithm that you can use to try to **minimize any function**, not just a cost function for linear regression.

  - min<sub>w<sub>1</sub>,...,w<sub>n</sub>,b</sub> J(w<sub>1</sub>,...,w<sub>n</sub>,b)

- Gradient descent:

  - Situation:

    - Have some function J(w, b)
    - Want min<sub>w, b</sub> J(w, b)

  - Outline:

    - Start with some w, b (e.g. set w=0, b=0) &larr; initial guesses
    - Keep changing w, b to reduce J(w, b) until we settle at or near a minimum

- Note:

  ![alt text](resources/notes/01.jpg)

## Gradient descent

## Implementing gradient descent

## Gradient descent intuition

## Learning rate

## Gradient descent for linear regression

## Running gradient descent

## Optional lab: Gradient descent
